{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![]( IMAGES/layers.png)\n",
    "<font size=2>*Deep Learning with Python*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![]( IMAGES/keras.png)\n",
    "<font size=2>*Hands-on Machine Learning*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When is a sequence model needed?\n",
    "\n",
    "France won the 2018 world cup, The French president was present and he congratulated --------- national team for their win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Sequence Models are ones where we want the state to be memorized and passed on to the next step**\n",
    "\n",
    "Said differently, the order and the sequence of the data matter and should be remembered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 align=\"center\"> RNN for Time Series data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![]( IMAGES/timesequence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**But RNN have a major downside: they seem to \"forget\" this internal state when the steps/sequence becomes long**\n",
    "\n",
    "This is because of something called vanishing gradients, which is a common problem for vanilla deep convulutions\n",
    "\n",
    "We will peel this off in the next few slides`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**So what is this thing I am calling vanishing gradient and why is it a problem**\n",
    "\n",
    "- Machine Learning at it's core learns through an objective seeking method\n",
    "- The objective most of the times is to minimize error between a target and the prediction from the algorithm\n",
    "- This means finding the minimum of the error function (cost function), where total error is minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Here's what an RNN really looks like**\n",
    "![]( IMAGES/rnn_step_forward.png)\n",
    "<caption><center> <u>Andrew Ng </u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <b>Typical Cost Function</b></center>\n",
    "\n",
    "<caption><center> <u>Andrew Ng </u></center></caption>\n",
    "<img src=\"IMAGES/optimsurface.png\"style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "The goal is to learn the coeficients $W$ and $b$ by minimizing the cost function. \n",
    "\n",
    "We update the coeficient $W$ by applying the following formula  $ W = W - \\alpha \\text{ } dW$.  \n",
    "\n",
    "$\\alpha$ here is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "as part of the gradient descent method, partial derivatives otherwise known as gradients have to be computed for every pass of the data ( this is what is referred to as Back Propagation)\n",
    "\n",
    "![]( IMAGES/rnn_cell_backprop.png)\n",
    "\n",
    "\n",
    "<caption><center> <u>Andrew Ng </u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM(Long Short Term Memory) are a special type of RNN that allows for longer sequences by using an attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Building a vanilla LSTM network**\n",
    "```python\n",
    "model = Sequential()\n",
    "'''The input to an LSTM layer has to be 3D tensor. The default LSTM layer output is 2D '''\n",
    "model.add(LSTM(32, input_shape=(NUM_TIMESTEPS,features )))## The shape of each input sample is defined in 1st layer only\n",
    "model.add(Dense(output))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How do we stack layers: It's all about managing input and output from one layer to another**\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "'''\n",
    "Input to LSTM layer has to always be 3D, since the output is by default is 2D. We need to make \n",
    "return_sequences=True in order to stack a second LSTM layer. \n",
    "This will make the output of the 1st LSTM layer return a 3D output, which can now be input to the 2nd LSTM layer\n",
    "dropout and Recurrent_dropout helps with regularizing the network and reduce overfitting. \n",
    "'''\n",
    "model.add(LSTM(32, input_shape=(NUM_TIMESTEPS,features ),dropout=0.3, recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(LSTM(64, input_shape=(NUM_TIMESTEPS,features ),dropout=0.3, recurrent_dropout=0.5))\n",
    "model.add(Dense(output))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "##### <[Ransac for data cleansing](_02_RANSAC.ipynb) |  [Home ](Index.ipynb)|  [LSTM Seq2Seq for Equipment Faults](_04_seq2seq.ipynb)>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
